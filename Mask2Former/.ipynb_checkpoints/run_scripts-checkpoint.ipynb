{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58a6aba0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ipanigra/Documents/Mask2Former/mask2former/modeling/pixel_decoder/ops\n",
      "running build\n",
      "running build_py\n",
      "running build_ext\n",
      "/home/ipanigra/Documents/riss2022/lib/python3.7/site-packages/torch/utils/cpp_extension.py:381: UserWarning: Attempted to use ninja as the BuildExtension backend but we could not find ninja.. Falling back to using the slow distutils backend.\n",
      "  warnings.warn(msg.format('we could not find ninja.'))\n",
      "running install\n",
      "running bdist_egg\n",
      "running egg_info\n",
      "writing MultiScaleDeformableAttention.egg-info/PKG-INFO\n",
      "writing dependency_links to MultiScaleDeformableAttention.egg-info/dependency_links.txt\n",
      "writing top-level names to MultiScaleDeformableAttention.egg-info/top_level.txt\n",
      "reading manifest file 'MultiScaleDeformableAttention.egg-info/SOURCES.txt'\n",
      "writing manifest file 'MultiScaleDeformableAttention.egg-info/SOURCES.txt'\n",
      "installing library code to build/bdist.linux-x86_64/egg\n",
      "running install_lib\n",
      "creating build/bdist.linux-x86_64/egg\n",
      "creating build/bdist.linux-x86_64/egg/modules\n",
      "copying build/lib.linux-x86_64-3.7/modules/__init__.py -> build/bdist.linux-x86_64/egg/modules\n",
      "copying build/lib.linux-x86_64-3.7/modules/ms_deform_attn.py -> build/bdist.linux-x86_64/egg/modules\n",
      "copying build/lib.linux-x86_64-3.7/MultiScaleDeformableAttention.cpython-37m-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/egg\n",
      "creating build/bdist.linux-x86_64/egg/functions\n",
      "copying build/lib.linux-x86_64-3.7/functions/__init__.py -> build/bdist.linux-x86_64/egg/functions\n",
      "copying build/lib.linux-x86_64-3.7/functions/ms_deform_attn_func.py -> build/bdist.linux-x86_64/egg/functions\n",
      "byte-compiling build/bdist.linux-x86_64/egg/modules/__init__.py to __init__.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/modules/ms_deform_attn.py to ms_deform_attn.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/functions/__init__.py to __init__.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/functions/ms_deform_attn_func.py to ms_deform_attn_func.cpython-37.pyc\n",
      "creating stub loader for MultiScaleDeformableAttention.cpython-37m-x86_64-linux-gnu.so\n",
      "byte-compiling build/bdist.linux-x86_64/egg/MultiScaleDeformableAttention.py to MultiScaleDeformableAttention.cpython-37.pyc\n",
      "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying MultiScaleDeformableAttention.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying MultiScaleDeformableAttention.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying MultiScaleDeformableAttention.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying MultiScaleDeformableAttention.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "writing build/bdist.linux-x86_64/egg/EGG-INFO/native_libs.txt\n",
      "zip_safe flag not set; analyzing archive contents...\n",
      "__pycache__.MultiScaleDeformableAttention.cpython-37: module references __file__\n",
      "creating 'dist/MultiScaleDeformableAttention-1.0-py3.7-linux-x86_64.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
      "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
      "Processing MultiScaleDeformableAttention-1.0-py3.7-linux-x86_64.egg\n",
      "removing '/home/ipanigra/Documents/riss2022/lib/python3.7/site-packages/MultiScaleDeformableAttention-1.0-py3.7-linux-x86_64.egg' (and everything under it)\n",
      "creating /home/ipanigra/Documents/riss2022/lib/python3.7/site-packages/MultiScaleDeformableAttention-1.0-py3.7-linux-x86_64.egg\n",
      "Extracting MultiScaleDeformableAttention-1.0-py3.7-linux-x86_64.egg to /home/ipanigra/Documents/riss2022/lib/python3.7/site-packages\n",
      "MultiScaleDeformableAttention 1.0 is already the active version in easy-install.pth\n",
      "\n",
      "Installed /home/ipanigra/Documents/riss2022/lib/python3.7/site-packages/MultiScaleDeformableAttention-1.0-py3.7-linux-x86_64.egg\n",
      "Processing dependencies for MultiScaleDeformableAttention==1.0\n",
      "Finished processing dependencies for MultiScaleDeformableAttention==1.0\n",
      "/home/ipanigra/Documents/Mask2Former\n"
     ]
    }
   ],
   "source": [
    "%cd /home/ipanigra/Documents/Mask2Former/mask2former/modeling/pixel_decoder/ops\n",
    "!python setup.py build install\n",
    "%cd ../../../../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf07419e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mask2former import add_maskformer2_config\n",
    "from automate_estimate_grnd_plane_segmentation import estimate_ground_plane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dedd9a96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ipanigra/Documents/Mask2Former'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6fd1530",
   "metadata": {},
   "outputs": [],
   "source": [
    "# common libraries\n",
    "import numpy as np\n",
    "import os\n",
    "import argparse\n",
    "from scipy.spatial import transform\n",
    "import shutil\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import cv2\n",
    "import torch, torchvision\n",
    "\n",
    "# detectron2 imports\n",
    "import detectron2\n",
    "from detectron2.utils.visualizer import _PanopticPrediction\n",
    "from detectron2.data import MetadataCatalog\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.utils.visualizer import Visualizer, ColorMode\n",
    "from detectron2.projects.deeplab import add_deeplab_config\n",
    "coco_metadata = MetadataCatalog.get(\"mapillary_vistas_panoptic_val\")\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.utils.logger import setup_logger\n",
    "setup_logger()\n",
    "setup_logger(name=\"mask2former\")\n",
    "\n",
    "# import Mask2Former\n",
    "from mask2former import add_maskformer2_config\n",
    "\n",
    "# other scripts\n",
    "from utils import filter_3d_points\n",
    "from utils import get_subdir\n",
    "\n",
    "\n",
    "def init_model():\n",
    "    cfg = get_cfg()\n",
    "    add_deeplab_config(cfg)\n",
    "    add_maskformer2_config(cfg)\n",
    "    # coco\n",
    "    # cfg.merge_from_file(\"configs/coco/panoptic-segmentation/swin/maskformer2_swin_large_IN21k_384_bs16_100ep.yaml\")\n",
    "    # cfg.MODEL.WEIGHTS = 'https://dl.fbaipublicfiles.com/maskformer/mask2former/coco/panoptic/maskformer2_swin_large_IN21k_384_bs16_100ep/model_final_f07440.pkl'\n",
    "    # mapillary vistas\n",
    "    cfg.merge_from_file(\"configs/mapillary-vistas/panoptic-segmentation/swin/maskformer2_swin_large_IN21k_384_bs16_300k.yaml\")\n",
    "    cfg.MODEL.WEIGHTS = 'https://dl.fbaipublicfiles.com/maskformer/mask2former/mapillary_vistas/panoptic/maskformer2_swin_large_IN21k_384_bs16_300k/model_final_132c71.pkl'\n",
    "    # cityscapes\n",
    "    # cfg.merge_from_file(\"configs/cityscapes/semantic-segmentation/swin/maskformer2_swin_small_bs16_90k.yaml\")\n",
    "    # cfg.MODEL.WEIGHTS = 'https://dl.fbaipublicfiles.com/maskformer/mask2former/cityscapes/semantic/maskformer2_swin_tiny_bs16_90k/model_final_2d58d4.pkl'\n",
    "    cfg.MODEL.MASK_FORMER.TEST.SEMANTIC_ON = True\n",
    "    cfg.MODEL.MASK_FORMER.TEST.INSTANCE_ON = True\n",
    "    cfg.MODEL.MASK_FORMER.TEST.PANOPTIC_ON = True\n",
    "    predictor = DefaultPredictor(cfg)\n",
    "    return cfg, predictor\n",
    "\n",
    "def read_images(path, subdir = 'sparse', file = 'images.txt'):\n",
    "    \"\"\"\n",
    "    input: root path\n",
    "    returns: dictionaries of image file names to their colmap IDs\n",
    "                             image file names to their 3D poses\n",
    "                             image file names to the 2D features \n",
    "    \"\"\"     \n",
    "    img2pose = {}\n",
    "    imgto2dfeatures = {}\n",
    "    img2id = {}\n",
    "    path  = os.path.join(path, subdir, file)\n",
    "    with open(path) as f:\n",
    "        lines = f.read().splitlines()\n",
    "\n",
    "    # Image list with two lines of data per image:\n",
    "    #   IMAGE_ID, QW, QX, QY, QZ, TX, TY, TZ, CAMERA_ID, NAME\n",
    "    #   POINTS2D[] as (X, Y, POINT3D_ID)\n",
    "    for i,line in enumerate(lines):\n",
    "        if line[0] == '#':\n",
    "            continue\n",
    "        else:\n",
    "            if i % 2 == 0:\n",
    "                fields = line.split(' ')\n",
    "                #NAME\n",
    "                image_name = fields[-1]\n",
    "                #IMAGE_ID\n",
    "                image_id = int(fields[0])\n",
    "                quat = fields[1:5]\n",
    "                quaternion = np.array([float(q) for q in quat])\n",
    "                trans = fields[5:8]\n",
    "                translation = np.array([float(t) for t in trans])\n",
    "                camera_id = int(fields[8])\n",
    "                #maps the image_name (jpg file) to it's pose\n",
    "                img2pose[image_name] = [quaternion, translation, image_id, camera_id]\n",
    "                #maps the name to an image id\n",
    "                img2id[image_name] = image_id\n",
    "            else:\n",
    "                fields = line.split(' ')\n",
    "                points_2d = np.array([float(pt) for pt in fields])\n",
    "                points_2d = np.reshape(points_2d, (-1, 3))\n",
    "                #maps the name to 2d points in the image\n",
    "                imgto2dfeatures[image_name] = points_2d\n",
    "    return img2pose, img2id, imgto2dfeatures\n",
    "\n",
    "def get_mask(predictor, im_path, cfg):\n",
    "    im = cv2.imread(im_path)\n",
    "    sem_seg = predictor(im)[\"sem_seg\"].to('cpu')\n",
    "    \n",
    "    # pred = _PanopticPrediction(panoptic_seg.to(\"cpu\"), segments_info, MetadataCatalog.get(cfg.DATASETS.TRAIN[0]))\n",
    "\n",
    "    # classes = np.array([x[1]['category_id'] for x in pred.semantic_masks()])\n",
    "    # instances = np.array([x[0] for x in pred.semantic_masks()])\n",
    "    stuff_classes = MetadataCatalog.get(cfg.DATASETS.TRAIN[0]).stuff_classes\n",
    "    \n",
    "    idx = (np.arange(len(stuff_classes)) == stuff_classes.index('Road'))\n",
    "    stuff_instances = sem_seg[idx]\n",
    "    \n",
    "    if len(stuff_instances) == 0:\n",
    "        pred_mask = np.ones_like(im)[:, :, 0]\n",
    "    else:\n",
    "        instances = stuff_instances\n",
    "        pred_mask = instances[0]\n",
    "        pred_mask = pred_mask.numpy().astype(int)\n",
    "    return pred_mask\n",
    "\n",
    "def read_predictions(args, parent, imgto2dfeatures, cfg, predictor):\n",
    "    \"\"\"\n",
    "    get the 3d ID of the ground points from image segmentations\n",
    "    \"\"\"\n",
    "    total_ground_points = []\n",
    "    for image_name, points_2d in imgto2dfeatures.items():\n",
    "        if os.path.exists(os.path.join(args.ground_path, parent, image_name)):\n",
    "            pred_mask = cv2.imread(os.path.join(args.ground_path, parent, image_name))\n",
    "            pred_mask = pred_mask[:, :, 0]\n",
    "            # print(os.path.join(args.ground_path, parent, parent))\n",
    "        else:\n",
    "            # print(os.path.join(args.ground_path, parent))\n",
    "            img_path = os.path.join(args.image_path, parent, image_name)\n",
    "            pred_mask = get_mask(predictor, img_path, cfg)\n",
    "            pred_mask = pred_mask * 255\n",
    "            os.makedirs(os.path.join(args.ground_path, parent, os.path.dirname(image_name)), exist_ok=True)    \n",
    "            cv2.imwrite(os.path.join(args.ground_path, parent, image_name), np.stack([pred_mask, pred_mask, pred_mask], axis = 2))            \n",
    "        image_points = points_2d[points_2d[:, 2] != -1]\n",
    "        image_points = image_points.astype(int)\n",
    "        image_points[:, [1, 0]] = image_points[:, [0, 1]]\n",
    "        image_points_bool = pred_mask[image_points[:, 0], image_points[:, 1]]\n",
    "        ground_points = image_points[image_points_bool > 0]\n",
    "        if len(ground_points) != 0:\n",
    "            total_ground_points.append(ground_points[:, 2])\n",
    "    return np.concatenate(total_ground_points)\n",
    "\n",
    "def remove_outlier(points3d_instance):\n",
    "    nbrs = NearestNeighbors(n_neighbors = 5)\n",
    "    X = points3d_instance[:, 1:4]\n",
    "    nbrs.fit(X)\n",
    "    distances, indexes = nbrs.kneighbors(X)\n",
    "    distances = distances[:, 1:]\n",
    "    mean = np.mean(distances.mean(axis = 1))\n",
    "    std = np.std(distances.mean(axis = 1))\n",
    "    inlier_index = distances.mean(axis = 1) < (mean)\n",
    "    return points3d_instance[inlier_index]\n",
    "\n",
    "def rotation_matrix_from_vectors(vec1, vec2):\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    Find the rotation matrix that aligns vec1 to vec2\n",
    "    :param vec1: A 3d \"source\" vector\n",
    "    :param vec2: A 3d \"destination\" vector\n",
    "    :return mat: A transform matrix (3x3) which when applied to vec1, aligns it with vec2.\n",
    "    \"\"\"\n",
    "    a, b = (vec1 / np.linalg.norm(vec1)).reshape(3), (vec2 / np.linalg.norm(vec2)).reshape(3)\n",
    "    v = np.cross(a, b)\n",
    "    c = np.dot(a, b)\n",
    "    s = np.linalg.norm(v)\n",
    "    kmat = np.array([[0, -v[2], v[1]], [v[2], 0, -v[0]], [-v[1], v[0], 0]])\n",
    "    rotation_matrix = np.eye(3) + kmat + kmat.dot(kmat) * ((1 - c) / (s ** 2))\n",
    "    return rotation_matrix\n",
    "\n",
    "def align_with_plane(coef):\n",
    "    \"given parameters of a plane, provide the transformation to make things align with it\"\n",
    "    tf1 = np.eye(4)\n",
    "    tf1[2, 3] = coef[-1] /coef[2] \n",
    "    vt = rotation_matrix_from_vectors(coef[:3], np.array([0,0,1]))\n",
    "    tf2 = np.eye(4)\n",
    "    tf2[:3, :3] = vt[:3, :3]\n",
    "    tf = tf2 @ tf1\n",
    "    return tf\n",
    "\n",
    "def normalize(colmap_3d_one_cam):\n",
    "    minimum = np.min(colmap_3d_one_cam[:, 0])\n",
    "    colmap_3d_one_cam[:, 0] = colmap_3d_one_cam[:, 0] - minimum\n",
    "    maximum = np.max(colmap_3d_one_cam[:, 0])\n",
    "    colmap_3d_one_cam[:, 0] = colmap_3d_one_cam[:, 0] * 1/ maximum\n",
    "    \n",
    "    minimum = np.min(colmap_3d_one_cam[:, 1])\n",
    "    colmap_3d_one_cam[:, 1] = colmap_3d_one_cam[:, 1] - minimum\n",
    "    maximum = np.max(colmap_3d_one_cam[:, 1])\n",
    "    colmap_3d_one_cam[:, 1] = colmap_3d_one_cam[:, 1] * 1/ maximum\n",
    "    \n",
    "    minimum = np.min(colmap_3d_one_cam[:, 2])\n",
    "    colmap_3d_one_cam[:, 2] = colmap_3d_one_cam[:, 2] - minimum\n",
    "    maximum = np.max(colmap_3d_one_cam[:, 2])\n",
    "    if maximum != 0:\n",
    "        colmap_3d_one_cam[:, 2] = colmap_3d_one_cam[:, 2] * 1/ maximum\n",
    "    return colmap_3d_one_cam\n",
    "\n",
    "def transform_cam(tf, lines):\n",
    "    # Image list with two lines of data per image:\n",
    "    #   IMAGE_ID, QW, QX, QY, QZ, TX, TY, TZ, CAMERA_ID, NAME\n",
    "    #   POINTS2D[] as (X, Y, POINT3D_ID)\n",
    "    for i,line in enumerate(lines):\n",
    "        if line[0] == '#':\n",
    "            continue\n",
    "        else:\n",
    "            if i % 2 == 0:\n",
    "                fields = line.split(' ')\n",
    "                \n",
    "                #get the image pose information\n",
    "                quat = fields[1:5]\n",
    "                quat = np.array([float(q) for q in quat])\n",
    "                trans = fields[5:8]\n",
    "                t = np.array([float(t) for t in trans])    \n",
    "                \n",
    "                #get pose in world frame\n",
    "                quat = [quat[1], quat[2], quat[3], quat[0]]\n",
    "                R = transform.Rotation.from_quat(quat)\n",
    "                R2 = R.as_matrix().T\n",
    "                tvec2 = -R.as_matrix().T.dot(t) \n",
    "            \n",
    "                # transform the pose into the new coordinate\n",
    "                tf2 = np.eye(4)\n",
    "                tf2[:3, :3] = R2\n",
    "                tf2[:3, 3] = tvec2\n",
    "                tf2 = tf @ tf2 \n",
    "                \n",
    "                #pose in world frame still\n",
    "                R2 = tf2[:3, :3]\n",
    "                tvec2 = tf2[:3, 3]\n",
    "                \n",
    "                #save rotation matrix in ego frame\n",
    "                quat = transform.Rotation.from_matrix(R2.T)\n",
    "                quat = quat.as_quat()\n",
    "                quat = [quat[3], quat[0], quat[1], quat[2]]\n",
    "                quat = [str(q) for q in quat]\n",
    "                fields[1:5] = quat\n",
    "                \n",
    "                #save translation matrix in ego frame\n",
    "                tvec2 = -R2.T.dot(tvec2) \n",
    "                trans = [str(t) for t in tvec2.tolist()]\n",
    "                fields[5:8] = trans\n",
    "                \n",
    "                lines[i] = ' '.join(fields)\n",
    "    return lines\n",
    "\n",
    "\n",
    "def view_points(points: np.ndarray, view: np.ndarray, normalize: bool) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    This is a helper class that maps 3d points to a 2d plane. It can be used to implement both perspective and\n",
    "    orthographic projections. It first applies the dot product between the points and the view. By convention,\n",
    "    the view should be such that the data is projected onto the first 2 axis. It then optionally applies a\n",
    "    normalization along the third dimension.\n",
    "\n",
    "    For a perspective projection the view should be a 3x3 camera matrix, and normalize=True\n",
    "    For an orthographic projection with translation the view is a 3x4 matrix and normalize=False\n",
    "    For an orthographic projection without translation the view is a 3x3 matrix (optionally 3x4 with last columns\n",
    "     all zeros) and normalize=False\n",
    "\n",
    "    :param points: <np.float32: 3, n> Matrix of points, where each point (x, y, z) is along each column.\n",
    "    :param view: <np.float32: n, n>. Defines an arbitrary projection (n <= 4).\n",
    "        The projection should be such that the corners are projected onto the first 2 axis.\n",
    "    :param normalize: Whether to normalize the remaining coordinate (along the third axis).\n",
    "    :return: <np.float32: 3, n>. Mapped point. If normalize=False, the third coordinate is the height.\n",
    "    \"\"\"\n",
    "\n",
    "    assert view.shape[0] <= 4\n",
    "    assert view.shape[1] <= 4\n",
    "    assert points.shape[0] == 3\n",
    "\n",
    "    viewpad = np.eye(4)\n",
    "    viewpad[:view.shape[0], :view.shape[1]] = view\n",
    "\n",
    "    nbr_points = points.shape[1]\n",
    "\n",
    "    # Do operation in homogenous coordinates.\n",
    "    points = np.concatenate((points, np.ones((1, nbr_points))))\n",
    "    points = np.dot(viewpad, points)\n",
    "    points = points[:3, :]\n",
    "\n",
    "    if normalize:\n",
    "        # points = points / points[2:3, :].repeat(3, 0).reshape(3, nbr_points)\n",
    "        points = points / np.linalg.norm(points[:3, :], axis = 0, keepdims=True).repeat(3, 0).reshape(3, nbr_points)\n",
    "    return points\n",
    "\n",
    "def dist(data, coef):\n",
    "    A = np.ones((data.shape[0], 1))\n",
    "    A = np.concatenate([data, A], axis = 1)\n",
    "    d = np.abs(np.dot(A, coef)) / np.sqrt(np.sum((coef ** 2)[:3]))\n",
    "    return d\n",
    "\n",
    "def fit_plane(data):\n",
    "    A = np.ones((data.shape[0], 1))\n",
    "    A = np.concatenate([data, A], axis = 1)\n",
    "    u, s, vt = np.linalg.svd(A, full_matrices=False)\n",
    "    coef = vt[3, :]    \n",
    "    return coef\n",
    "\n",
    "def ransac(data, threshold = 0.001):\n",
    "    best_cnt = 0\n",
    "    iteration = 2000\n",
    "    print(data.shape)\n",
    "    for i in range(iteration):\n",
    "        sample = data[np.random.randint(data.shape[0], size=4), :]\n",
    "        coef = fit_plane(sample)\n",
    "        cnt = np.sum(dist(data, coef) < threshold)\n",
    "        if cnt > best_cnt:\n",
    "            best_cnt = cnt\n",
    "            best_coef = coef\n",
    "    return best_coef\n",
    "\n",
    "def create_image_pose(img2pose):\n",
    "    img_pose = []\n",
    "    for img_fc in img2pose.keys():\n",
    "        q = img2pose[img_fc][0]\n",
    "        quat = [q[1], q[2], q[3], q[0]]\n",
    "        R = transform.Rotation.from_quat(quat)\n",
    "        t = img2pose[img_fc][1]\n",
    "\n",
    "        #point in world frame\n",
    "        colmap_pt = - R.as_matrix().T.dot(t) \n",
    "        img_pose.append(colmap_pt)\n",
    "    img_pose = np.array(img_pose)\n",
    "\n",
    "    \n",
    "    #make it into colmap's format\n",
    "    r, c = img_pose.shape\n",
    "    rng = np.random.default_rng(12345)\n",
    "    rints = rng.integers(low=1, high=1000000, size=img_pose.shape[0])\n",
    "    img_pose = np.hstack([rints[:, np.newaxis], img_pose, np.zeros((r, 1)), np.ones((r, 1)) * 255, np.zeros((r, 1))])\n",
    "    r, c = img_pose.shape\n",
    "    img_pose = np.hstack([img_pose, np.ones((r, 13-c))])\n",
    "    \n",
    "    return img_pose\n",
    "\n",
    "def create_plane_points(img_pose):\n",
    "    x_range = max(img_pose[:, 1]) - min(img_pose[:, 1])\n",
    "    y_range = max(img_pose[:, 2]) - min(img_pose[:, 2])\n",
    "    x = np.linspace(min(img_pose[:, 1]) - x_range * 0.1, max(img_pose[:, 1])  + x_range * 0.1, 200)\n",
    "    y = np.linspace(min(img_pose[:, 2])- y_range * 0.1, max(img_pose[:, 2])  + y_range * 0.1, 200)\n",
    "    xx, yy = np.meshgrid(x, y)\n",
    "    z = np.zeros_like(xx)\n",
    "    \n",
    "    plane = np.stack([xx.flatten(), yy.flatten(), z.flatten()], axis = 1)    \n",
    "    #make it into colmap's format\n",
    "    r, c = plane.shape\n",
    "    rng = np.random.default_rng(12345)\n",
    "    rints = rng.integers(low=1, high=1000000, size=plane.shape[0])\n",
    "    plane = np.hstack([rints[:, np.newaxis], plane, np.zeros((r, 1)), np.ones((r, 1)) * 255, np.zeros((r, 1))])\n",
    "    r, c = plane.shape\n",
    "    plane = np.hstack([plane, np.ones((r, 13-c))])\n",
    "    return plane    \n",
    "\n",
    "def gen_points_output(args, parent, points3d_final):\n",
    "    fmt = '%d', '%.18f', '%.18f', '%.18f', '%d', '%d', '%d', '%.18f', '%d', '%d', '%d', '%d', '%d'\n",
    "    np.savetxt(os.path.join(parent, args.output, 'points3D.txt'), points3d_final, fmt = fmt) \n",
    "  \n",
    "def gen_image_output(args, parent, tf):\n",
    "    #convert all poses of the images \n",
    "    path  = os.path.join(args.path, parent, 'images.txt')\n",
    "    with open(path) as f:\n",
    "        lines = f.read().splitlines()\n",
    "    lines = transform_cam(tf, lines)\n",
    "    with open(os.path.join(parent, args.output, 'images.txt') , 'w') as filehandle:\n",
    "        filehandle.writelines(\"%s\\n\" % place for place in lines)    \n",
    "\n",
    "  \n",
    "class Argument():\n",
    "    def __init__(self, path, image_path, output, ground_path):\n",
    "        self.path = path\n",
    "        self.image_path = image_path\n",
    "        self.output = output\n",
    "        self.ground_path = ground_path\n",
    "\n",
    "def estimate_ground_plane(path, image_path, output, ground_path):\n",
    "    args = Argument(path, image_path, output, ground_path)\n",
    "    cfg, predictor = init_model()\n",
    "    args.cfg = cfg\n",
    "    args.predictor = predictor \n",
    "\n",
    "    \n",
    "    #load the points\n",
    "    path  = os.path.join(args.path, 'points3D.txt')\n",
    "    #sparse point clouds have error, trac, etc. which we don't need\n",
    "    points3d_sparse = np.loadtxt(path, usecols = (0, 1, 2, 3, 4, 5, 6, 7))\n",
    "    r, c = points3d_sparse.shape\n",
    "    points3d_sparse = np.hstack([points3d_sparse, np.ones((r, 13-c))])\n",
    "\n",
    "\n",
    "    #get the pose of each imge\n",
    "    img2pose, img2id, imgto2dfeatures = read_images(args.path, subdir = '', file = 'images.txt')\n",
    "    img_pose = create_image_pose(img2pose)\n",
    "    \n",
    "    #get ground points\n",
    "    instance_points = read_predictions(args, '', imgto2dfeatures, args.cfg, args.predictor)\n",
    "    id3d = np.unique(instance_points)[:, np.newaxis]\n",
    "    points3d_instance = filter_3d_points(args.path, id3d, subdir = '')\n",
    "    points3d_instance = remove_outlier(points3d_instance)\n",
    "\n",
    "    #estimate the ground plane\n",
    "    coef = ransac(points3d_instance[:, 1:4])    \n",
    "    tf = align_with_plane(coef)\n",
    "\n",
    "    #transform both points and cameras\n",
    "    points3d_sparse[:, 1:4] = view_points(points3d_sparse[:, 1:4].T, tf, normalize=False).T\n",
    "    img_pose[:, 1:4] = view_points(img_pose[:, 1:4].T, tf, normalize=False).T\n",
    "\n",
    "    #add the plane points\n",
    "    plane = create_plane_points(img_pose)\n",
    "    points3d_final = np.concatenate([points3d_sparse, plane], axis = 0)\n",
    "\n",
    "    #create output\n",
    "    os.makedirs(args.output, exist_ok=True)    \n",
    "    shutil.copy2(os.path.join(args.path, 'cameras.txt'), os.path.join(args.output, 'cameras.txt'))\n",
    "    gen_points_output(args, '', points3d_final)\n",
    "    gen_image_output(args, '', tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "557464da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading config configs/mapillary-vistas/panoptic-segmentation/swin/../Base-MapillaryVistas-PanopticSegmentation.yaml with yaml.unsafe_load. Your machine may be at risk if the file contains malicious content.\n",
      "/home/ipanigra/Documents/riss2022/lib/python3.7/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[08/03 10:11:50 mask2former.modeling.transformer_decoder.mask2former_transformer_decoder]: \u001b[0mWeight format of MultiScaleMaskedTransformerDecoder have changed! Please upgrade your models. Applying automatic conversion now ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ipanigra/Documents/Mask2Former/mask2former/modeling/transformer_decoder/position_encoding.py:41: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  dim_t = self.temperature ** (2 * (dim_t // 2) / self.num_pos_feats)\n",
      "/home/ipanigra/Documents/Mask2Former/mask2former/maskformer_model.py:355: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  topk_indices = topk_indices // self.sem_seg_head.num_classes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(638, 3)\n"
     ]
    }
   ],
   "source": [
    "estimate_ground_plane('../../Downloads/clear_weather_stretch2/model_text_withqueries/','../../Downloads/clear_weather_stretch2/images','./output_withqueries','./gt_road_withqueries')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
